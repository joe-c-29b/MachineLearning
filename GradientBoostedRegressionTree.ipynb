{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Regression Tree algorithm: \n",
    "When to use?\n",
    "1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clfr = GradientBoostingClassifier(loss='log_loss',\n",
    "                                  learning_rate=0.1,\n",
    "                                  n_estimators=100,\n",
    "                                  subsample=1.0,\n",
    "                                  criterion='friedman_mse', \n",
    "                                  max_depth=None, \n",
    "                                  min_samples_split=2, \n",
    "                                  min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, \n",
    "                                  init=None,\n",
    "                                  max_features=None,\n",
    "                                  random_state=None, \n",
    "                                  max_leaf_nodes=None, \n",
    "                                  min_impurity_decrease=0.0, \n",
    "                                  ccp_alpha=0.0,\n",
    "                                  validation_fraction=0.1,\n",
    "                                  no_iter_no_change=None,\n",
    "                                  tol=1e-4,\n",
    "                                  verbose=0,\n",
    "                                  warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(loss='squared_error',\n",
    "                                  learning_rate=0.1,\n",
    "                                  n_estimators=100,\n",
    "                                  subsample=1.0,\n",
    "                                  criterion='friedman_mse', \n",
    "                                  max_depth=None, \n",
    "                                  min_samples_split=2, \n",
    "                                  min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, \n",
    "                                  init=None,\n",
    "                                  max_features=None,\n",
    "                                  random_state=None, \n",
    "                                  max_leaf_nodes=None, \n",
    "                                  min_impurity_decrease=0.0, \n",
    "                                  ccp_alpha=0.0,\n",
    "                                  alpha=0.9,\n",
    "                                  validation_fraction=0.1,\n",
    "                                  no_iter_no_change=None,\n",
    "                                  tol=1e-4,\n",
    "                                  verbose=0,\n",
    "                                  warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##METHODS\n",
    "reg.fit(X, y) #build a decision tree\n",
    "reg.get_params(deep=True) #get the parameters for this estimator\n",
    "reg.apply(X) #index of the leaf that each sample is predicted as\n",
    "reg.predict(X) #predict the target for the provided data\n",
    "reg.score(X, y, sample_weight=None) #Return the coefficient of determination of the prediction.\n",
    "reg.set_params() #Set the parameters of this estimator\n",
    "reg.staged_predict(X) #predict regression target at each stage for X\n",
    "\n",
    "##CLFR only method\n",
    "clfr.predict_proba(X[, check_input]) #Return probability estimates for the test data X.\n",
    "clfr.predict_log_proba(X) #same as above, but it is the log of probabilities\n",
    "clfr.decision_function(X) #compute decision function for X\n",
    "clfr.staged_decision_function(X) #compute decision function of X for each iteration\n",
    "clfr.staged_predict_proba(X) #predict class probabilities at each stage for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BOTH ATTRIBUTES\n",
    "reg.feature_importance_\n",
    "reg.oob_improvement_\n",
    "reg.train_score_\n",
    "reg.loss_\n",
    "reg.init_\n",
    "reg.n_estimators_\n",
    "reg.estimators_\n",
    "reg.n_features_in_\n",
    "reg.feature_names_in_\n",
    "reg.max_features_\n",
    "##CLASSIFIER ONLY ATTRIBUTES\n",
    "clfr.classes_\n",
    "clfr.n_classes_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS\n",
    "loss= loss function to be optimized\n",
    "    -'log_loss' (classifier only) binomial and multinomial deviance (same as used in logistic regression)\n",
    "    -'exponential' (classifier only) gradient boosting recovers the adaboost algorithm\n",
    "    -'squared_error' (regressor only)\n",
    "    -'absolute_error' (regressor only)\n",
    "    -'huber' (regressor only)\n",
    "    -'quantile' (regressor only)\n",
    "learning_rate= \n",
    "n_estimators= the number of trees to create\n",
    "subsample= fraction of samples to be used for fitting the individual base learners\n",
    "criterion= the function to measure the quality of the split\n",
    "    -'friedman_mse'\n",
    "    -'squared_error'\n",
    "max_depth= specify the depth you want the tree to stop at\n",
    "    -int\n",
    "    -None : nodes are expanded until all leaves are pure or contain less than min_samples_split specified\n",
    "min_samples_split= minimum number of samples required to split an internal node\n",
    "    -int\n",
    "    -float\n",
    "min_samples_leaf= minimum number of samples required to be at a leaf node\n",
    "    -int\n",
    "    -float\n",
    "min_weight_fraction_leaf= (float)minimum weighted fraction of the sum total of weights requred to be at a leaf node\n",
    "max_features= number of features to consider when looking for the best split\n",
    "    -int\n",
    "    -float\n",
    "    -'auto' all features\n",
    "    -'sqrt' square root of all features\n",
    "    -'log2' log2 of all features\n",
    "    -None : same as 'auto'\n",
    "init= estimator object that is used to compute the initial preditions, must provide fit and predict_proba\n",
    "    -estimator\n",
    "    -0\n",
    "validation_fraction= proportion of training data set aside to validate with if early stopping is in place (only use if n_iter_no_change is an int)\n",
    "n_iter_no_change= parameter for if you will be implementing early stopping\n",
    "tol= tolerance for the early stopping, threshold for if loss hasn't improved by tol for n_iter_no_change\n",
    "n_jobs= (default None) how the processing is going to occur\n",
    "    - -1 means using all available processors\n",
    "verbose= the amount of feedback messages to permit the system to output\n",
    "warm_start= True means to reuse the solution of te previous call to fit and add more estimators to the ensemble\n",
    "random_state= specify the random state for choosing instances and features\n",
    "max_leaf_nodes= grow the tree in the best-first fashion\n",
    "min_impurity_decrease= a node  will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "ccp_alpha= complexity parameter used for pruning\n",
    "\n",
    "REGRESSOR ONLY\n",
    "alpha= the alpha-quantile of the huber loss function and the quantile loss function (can only be used if loss= ;'huber' or 'quantile')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTRIBUTES\n",
    "feature_importances_ : breakdown of all features and their relative importances\n",
    "n_estimators_ : number of estimators as selected by early stopping, otherwise, whatever is set to n_estimators parameter\n",
    "estimator_ : the child estimator template used to create the collection of fitted sub-estimators; estimator used to grow the ensemble\n",
    "estimators_ : collection of fitted sub-estimators\n",
    "init_ : estimator that provides the initial predictions\n",
    "n_classes_ : number of classes\n",
    "n_features_in_ : number of features seen during the .fit()\n",
    "feature_names_in_ : names of the above features\n",
    "max_features_ : inferred value of max features\n",
    "oob_improvement_ : improvement in loss on the oob samples relative to the previous iteration\n",
    "oob_score_ : score of the training dataset obtained using oob estimate\n",
    "oob_decision_function_ : decision function used with oob on the training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
