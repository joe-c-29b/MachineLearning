{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree algorithm: creates a model to predict the value of a target variable by learning simple decision rules based on the training data\n",
    "-non-parametric\n",
    "-supervised learning\n",
    "\n",
    "When to use?\n",
    "1) If the goal is exploratory analysis\n",
    "2) when we want a simple model\n",
    "3) when entire dataset and features can be used\n",
    "4) when we have limited computational power\n",
    "5) when we are not worried about accuracy on future datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clfr = DecisionTreeClassifier(criterion='squared_error', \n",
    "                            splitter='best', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features=None, \n",
    "                            random_state=None, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, \n",
    "                            ccp_alpha=0.0,\n",
    "                            class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg = DecisionTreeRegressor(criterion='squared_error', \n",
    "                            splitter='best', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features=None, \n",
    "                            random_state=None, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, \n",
    "                            ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##METHODS\n",
    "reg.fit(X, y) #build a decision tree\n",
    "reg.get_params(deep=True) #get the parameters for this estimator\n",
    "reg.apply(X,[,check_input]) #index of the leaf that each sample is predicted as\n",
    "reg.cost_complexity_pruning_path(X, y, sample_weight=None) #compute the pruning path during Minimal cost complexity pruning\n",
    "reg.decision_path(X,[,check_input]) #decision path in the tree for a sample\n",
    "reg.get_depth() #depth of the decision tree\n",
    "reg.get_n_leaves() #number of leaves of the complete DT\n",
    "reg.predict(X) #predict the target for the provided data\n",
    "reg.score(X, y, sample_weight=None) #Return the coefficient of determination of the prediction.\n",
    "reg.set_params() #Set the parameters of this estimator\n",
    "\n",
    "##CLFR only method\n",
    "clfr.predict_proba(X[, check_input]) #Return probability estimates for the test data X.\n",
    "clfr.predict_log_proba(X) #same as above, but it is the log of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BOTH ATTRIBUTES\n",
    "reg.feature_importance_\n",
    "reg.max_features_\n",
    "reg.n_features_in_\n",
    "reg.feature_names_in_\n",
    "reg.n_outputs_\n",
    "reg.tree_\n",
    "##CLASSIFIER ONLY ATTRIBUTES\n",
    "clfr.classes_\n",
    "clfr.n_classes_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS\n",
    "criterion= the function to measure the quality of the split\n",
    "    -'squared_error' mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node\n",
    "    -'friedman_mse' mean squared error with Friedmanâ€™s improvement score for potential splits\n",
    "    -'absolute_error' mean absolute error, which minimizes the L1 loss using the median of each terminal node\n",
    "    -'poisson' reduction in Poisson deviance to find splits\n",
    "splitter= strategy for choosing the splits at each node\n",
    "    -'best'\n",
    "    -'random'\n",
    "max_depth= specify the depth you want the tree to stop at\n",
    "    -int\n",
    "    -None : nodes are expanded until all leaves are pure or contain less than min_samples_split specified\n",
    "min_samples_split= minimum number of samples required to split an internal node\n",
    "    -int\n",
    "    -float\n",
    "min_samples_leaf= minimum number of samples required to be at a leaf node\n",
    "    -int\n",
    "    -float\n",
    "min_weight_fraction_leaf= (float)minimum weighted fraction of the sum total of weights requred to be at a leaf node\n",
    "max_features= number of features to consider when looking for the best split\n",
    "    -int\n",
    "    -float\n",
    "    -'auto' all features\n",
    "    -'sqrt' square root of all features\n",
    "    -'log2' log2 of all features\n",
    "    -None : same as 'auto'\n",
    "random_state= specify the random state for choosing instances and features\n",
    "max_leaf_nodes= grow the tree in the best-first fashion\n",
    "min_impurity_decrease= a node  will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "ccp_alpha= complexity parameter used for pruning\n",
    "CLASSIFIER ONLY\n",
    "class_weight= weights associated with the classes in dictionary form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTRIBUTES\n",
    "feature_importance_ : ranking of features and their proportional importance\n",
    "max_features_ : inferred value of max_features\n",
    "n_features_in_ : number of features seen during the .fit()\n",
    "feature_names_in_ : names of the above features\n",
    "n_outputs_ : number of outputs when .fit() is performed\n",
    "tree_ : the tree object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
