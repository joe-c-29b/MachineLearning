Types:
  supervised | unsupervised | semisupervised | reinforcement learning
  online | batch learning (system trained by all available data)
  instance-based | model-based
  
ALGORITHM
K-Nearest Neighbors
  from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier

Decision Tree
  from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
  
Random Forest
  from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
  
Gradient Boosted Regression Tree
  from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier

Voting Classifier
  from sklearn.ensemble import VotingClassifier
  
  
  
Base Estimator
  from sklearn.base import BaseEstimator
  model = BaseEstimator()

Support Vector Machine
  from sklearn.svm import SVC
  model = SVC(gamma=, random_state=)

  from sklearn.svm import SVR
  model = SVR(epsilon=, random_state=)
  
  from sklearn.svm import LinearSVC
  model = LinearSVC()
  
  from sklearn.svm import LinearSVR
  model = LinearSVR(epsilon=, random_state=)
  
One Vs Rest
  from sklearn.multiclass import OneVsRestClassifier
  model = OneVsRestClassifier()

One Vs One
  from sklearn.multiclass import OneVsOneClassifier
  model = OneVsOneClassifier()

Linear Regression
  import sklearn.linear_model
  model = sklearn.linear_model.LinearRegression()
  
Stochastic Gradient Descent
  from sklearn.linear_model import SGDClassifier
  model = SGDClassifier(loss=, penalty=, random_state=)
  
  from sklearn.linear_model import SGDRegressor
  model = SGDRegressor(max_iter=, tol=, eta=, loss=, penalty=, random_state=)
  
Ridge Regression
  from sklearn.linear_model import Ridge
  model = Ridge(alpha=, solver=)

Least Absolute Shrinkage and Selection Operator Regression
  from sklearn.linear_model import Lasso
  model = Lasso(alpha=)

Elastic Net
  from sklearn.linear_model import ElasticNet
  model = ElasticNet(alpha=, 11_ratio=)
  
Boot-strap Aggregating
  from sklearn.ensemble import BaggingClassifier
  model = BaggingClassifier()

Adaptive Boosting
  from sklearn.ensemble import AdaBoostClassifier
  model = AdaBoostClassifier()






GridSearchCV
  from sklearn.model_selection import GridSearchCV
    GridSearchCV(estimator, 
      param_grid, 
      scoring=None, 
      n_jobs=None, 
      refit=True, 
      cv=None, 
      verbose=0, 
      pre_dispatch='2*n_jobs', 
      error_score=nan, 
      return_train_score=False)
      
      PARAMETERS - search over specified parameter values for an estimator in order to find the best combination of parameters for the estimator
      estimator= the estimator you are searching for; must have either a score function or 'scoring' must be passed 
      param_grid= dictionary with parameter names as keys and lists of parameter settings to try as values 
      scoring= strategy to evaluate the performance of the cross-validated model on the test set 
      n_jobs= parallel jobs running
        --1 means use all available processors
      refit= refit an estimator using the best found parameters on the whole dataset
      cv= cross-validation splitting strategy
        -None : default to use t5-fold cross validation
        -int: to specify the number of folds in a (Stratified)KFold
        -CV splitter
        -iter : yielding (train, test) splits as arrays of indices 
      verbose= how many describing messages the function will output 
        -1 : the computation time for each fold and parameter candidate is displayed
        -2 : the score is also displayed
        -3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation
      pre_dispatch= number of jobs that get dispatched during parallel execution; reducing can help avoid excessive RAM usage if that is an issue
        -None
        -int : exact number of jobs that are spawned
        -str : an expression of n_jobs; ex. '2*n_jobs'
      error_score=value to assign to the score if an error occurs during fitting
        -'raise' : the error is raised
        -int : FitFailedWarning is raised
        -np.nan : default, raises no error
      return_train_score= whether or not the training scores will be given in the results

      ATTRIBUTES
      cv_results_ : a dictionary with keys as column headers and values as columns
      best_estimator_ : estimator dubbed the best by the grid search (not available if refit=False)
      best_score_ : the mean cross-validated score of the best estimator(not available if refit is a function)
      best_params_ : parameter setting that gave the best results on the hold out data
      best_index_ : index that corresponds to the best candidate parameter setting in the cv_results_
      scorer_ : function used on the held out data to choose the best parameters
      n_splits_ : number of cross-validation splits (folds)
      refit_time_ : seconds used for refitting the best model (only if refit=True)
      multimetric_ : whether or not the scorers compute several metrics
      classes_ : class labels
      n_features_in_ : number of features seen during fit
      feature_names_in_ : names of the above features
      
      METHODS
      decision_function(X) #call on the estimator with the best found parameters
      fit() 
      get_params([deep]) #get the parameters for this estimator
      inverse_transform(Xt) #call on the estimator with the best found params
      predict(X)
      predict_log_proba(X)
      predict_proba(X)
      score(X[,y]) #return the score on the given data, if the estimator has been refit
      score_samples(X) #call on the estimator with the best found parameters
      set_params()
      transform(X) #call on the estimator with the best found parameters
      
      

Train-Test Split
  from sklearn.model_selection import train_test_split
  train_set, test_set = train_test_split(data, test_size=, random_state=)
Stratified Shuffle Split
  from sklearn.model_selection import StratifiedShuffleSplit
  split = StratifiedShuffleSplit(data, n_splits=, test_size=, random_state=)
  
  
COMMON PROBLEMS:
Sampling Noise - nonrepresentative data as a result of chance
Sampling bias - nonrepresentative data as a result of a flawed sampling method
Skewed dataset - when some classes are more frequent than others
Computational complexity - 



1. look at the big picture
2. get the data
3. discover and visualize the data to gain insights
4. prepare the data for ML algorithms
5. select a model and train it
6. fine tune your model
7. present your solution
8. launch, monitor, and maintain


ALSO DO SAMPLES OF IMPUTERS, ENCODERS, TRANSFORMERS, FEATURE SCALING, PIPELINES, CROSS-VALIDATION, STANDARDSCALER, POLYNOMIALFEATURES

ALSO DO SAMPLES FOR GRIDSEARCHCV, RANDOMIZEDSEARCHCV, CONFUSIONMATRIX, EARLY STOPPING, KERNEL TRICK

ALSO DO SAMPLES FOR SCORING, ROC CURVE, SVD, GRADIENTDESCENT


