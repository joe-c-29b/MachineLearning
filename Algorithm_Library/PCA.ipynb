{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA) - an algorithm used for dimensionality reduction.\n",
    "Randomized PCA - svd_slover is set to 'randomized, finds the 1st approximation of the first PC d; significantly faster, ist the default for 'auto' if the group is greater than 500 and the dimensions is less than 80% of that.\n",
    "Incremental PCA - allows you to split the training set into mini_batches and feed one batch at a time\n",
    "Kernel PCA - applying the kernel trick of enableing nonlinear classification/regression using PCA; good for preserving clusters of instances after projecton and unrolling datasets that are close to a twisted manifold\n",
    "\n",
    "\n",
    "Finds the components vectors from the center of the dataset that capture the most variance, to the n number of components specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "pca = PCA(\n",
    "  n_components=None, \n",
    "  copy=True, \n",
    "  whiten=False, \n",
    "  svd_solver='auto', \n",
    "  tol=0.0, \n",
    "  iterated_power='auto', \n",
    "  n_oversamples=10, \n",
    "  power_iteration_normalizer='auto', \n",
    "  random_state=None\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METHODS\n",
    "pca.fit(X[,y]) #fit the model\n",
    "pca.fit_transform(X[,y]) #fit and apply the dimensionality reduction on\n",
    "pca.get_covariance() #compute data covariance with the generative model\n",
    "pca.get_features_names_out([input_features]) #get feature names for transformation\n",
    "pca.get_params([deep]) #get parameters for the estimator\n",
    "pca.get_precision() #get precision matrix with the generative model\n",
    "pca.score(X[,y]) #mean lo-likelihood of all samples\n",
    "pca.score_samples(X) #log-likelihood of ea sample\n",
    "pca.set_output(*[, transform]) #set output container\n",
    "pca.set_params(**params) #set the parameters of the estimator\n",
    "pca.transform(X) #apply dimensionality reduction\n",
    "pca.inverse_transform(X) #reverse the .transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTRIBUTES\n",
    "pca.components_\n",
    "pca.explained_variance_\n",
    "pca.explained_variance_ratio_\n",
    "pca.singular_values_\n",
    "pca.mean_\n",
    "pca.n_components_\n",
    "pca.n_features_\n",
    "pca.n_samples_\n",
    "pca.noise_variance_\n",
    "pca.n_features_in_\n",
    "pca.features_names_in_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS\n",
    "\n",
    "        -n_components : number of components you want to keep\n",
    "            -min(n_samples, n_features)  Minka's MLE is used to guess dimensions\n",
    "            -'mle'\n",
    "            -int\n",
    "            -None (default) will be equal to min(n_samples, n_features)-1\n",
    "        -copy : \n",
    "            -False data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead\n",
    "            -True (default)\n",
    "        -whiten :\n",
    "            -True the components_ vectors are multiplied by the square root of n_samples and then divided by singular values to ensure uncorrelated outputs with unit component-wise variances; whitening will remove some info from the transformed signal, but can sometimes improve predictive accuracy\n",
    "            -False (default)\n",
    "        -svd_solver : \n",
    "            -'auto' (default) solver is selected based on X.shape and n_components\n",
    "            -'full' run exact full SVD calling the standard LAPACK solver\n",
    "            -'arpack' run SVD truncated to n_components calling ARPACK solver (requires 0 < n_components < min(X.shape))\n",
    "            -'randomized' run randomized SVD by Halko et al.\n",
    "        -tol : Tolerance for singular values computed by svd_solver == ‘arpack’. Must be of range [0.0, infinity)\n",
    "        -iterated_power : Number of iterations for the power method computed by svd_solver == ‘randomized’. Must be of range [0, infinity).\n",
    "        -n_oversamples : This parameter is only relevant when svd_solver=\"randomized\". It corresponds to the additional number of random vectors to sample the range of X so as to ensure proper conditioning.\n",
    "        -power_iteration_normalizer : Power iteration normalizer for randomized SVD solver. Not used by ARPACK. \n",
    "        -random_state : Used when the ‘arpack’ or ‘randomized’ solvers are used. Pass an int for reproducible results across multiple function calls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTRIBUTES\n",
    "\n",
    "        -components_ : Principal axes in feature space, representing the directions of maximum variance in the data. \n",
    "        -explained_variance_ : The amount of variance explained by each of the selected components.\n",
    "        -explained_variance_ratio_ : Percentage of variance explained by each of the selected components.\n",
    "        -singular_values_ : The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space.\n",
    "        -mean_ : Per-feature empirical mean, estimated from the training set.\n",
    "        -n_components_ : The estimated number of components.\n",
    "        -n_features_ : number of features in the training set\n",
    "        -n_samples_ : number of samples in the training set\n",
    "        -noise_variance_ : The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999.\n",
    "        -n_features_in_ : number of features seen during fit()\n",
    "        -features_names_in_ : names of the above features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
