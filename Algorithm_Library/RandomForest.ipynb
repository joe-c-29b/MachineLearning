{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Forest algorithm: creates an ensemble of trees, each tree is built from a sample drawn w/ replacement; additionally, when splitting each node, the best split is found either from all input features or a random subset\n",
    "-non-parametric\n",
    "-supervised learning\n",
    "\n",
    "When to use?\n",
    "1) There is an overfitting problem with your decision tree\n",
    "2) Need to detect non-linearity\n",
    "3) Data is highly dimensional and outliers are a plenty throughout\n",
    "4) Struggling with the current bias of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfr = RandomForestClassifier(n_estimators=100,\n",
    "                            criterion='gini', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features='sqrt', \n",
    "                            random_state=None, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, \n",
    "                            ccp_alpha=0.0,\n",
    "                            class_weight=None,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=False,\n",
    "                            n_jobs=None,\n",
    "                            verbose=0,\n",
    "                            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators=100,\n",
    "                            criterion='gini', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features='sqrt', \n",
    "                            random_state=None, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_decrease=0.0, \n",
    "                            ccp_alpha=0.0,\n",
    "                            class_weight=None,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=False,\n",
    "                            n_jobs=None,\n",
    "                            verbose=0,\n",
    "                            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##METHODS\n",
    "reg.fit(X, y) #build a decision tree\n",
    "reg.get_params(deep=True) #get the parameters for this estimator\n",
    "reg.apply(X,[,check_input]) #index of the leaf that each sample is predicted as\n",
    "reg.decision_path(X,[,check_input]) #decision path in the tree for a sample\n",
    "reg.predict(X) #predict the target for the provided data\n",
    "reg.score(X, y, sample_weight=None) #Return the coefficient of determination of the prediction.\n",
    "reg.set_params() #Set the parameters of this estimator\n",
    "\n",
    "##CLFR only method\n",
    "clfr.predict_proba(X[, check_input]) #Return probability estimates for the test data X.\n",
    "clfr.predict_log_proba(X) #same as above, but it is the log of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BOTH ATTRIBUTES\n",
    "reg.feature_importance_\n",
    "reg.estimator_\n",
    "reg.estimators_\n",
    "reg.n_features_in_\n",
    "reg.feature_names_in_\n",
    "reg.n_outputs_\n",
    "reg.oob_score_\n",
    "##CLASSIFIER ONLY ATTRIBUTES\n",
    "clfr.classes_\n",
    "clfr.n_classes_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS\n",
    "n_estimators= the number of trees to create\n",
    "criterion= the function to measure the quality of the split\n",
    "    -'gini'\n",
    "    -'entropy'\n",
    "    -'log_loss'\n",
    "max_depth= specify the depth you want the tree to stop at\n",
    "    -int\n",
    "    -None : nodes are expanded until all leaves are pure or contain less than min_samples_split specified\n",
    "min_samples_split= minimum number of samples required to split an internal node\n",
    "    -int\n",
    "    -float\n",
    "min_samples_leaf= minimum number of samples required to be at a leaf node\n",
    "    -int\n",
    "    -float\n",
    "min_weight_fraction_leaf= (float)minimum weighted fraction of the sum total of weights requred to be at a leaf node\n",
    "max_features= number of features to consider when looking for the best split\n",
    "    -int\n",
    "    -float\n",
    "    -'auto' all features\n",
    "    -'sqrt' square root of all features\n",
    "    -'log2' log2 of all features\n",
    "    -None : same as 'auto'\n",
    "bootstrap= whether or not bootstrap sampling is being done \n",
    "oob_sscore= whether or not out-of-bag samples are being used (only if bootstrap=True)\n",
    "n_jobs= (default None) how the processing is going to occur\n",
    "    - -1 means using all available processors\n",
    "verbose= the amount of feedback messages to permit the system to output\n",
    "warm_start= True means to reuse the solution of te previous call to fit and add more estimators to the ensemble\n",
    "random_state= specify the random state for choosing instances and features\n",
    "max_leaf_nodes= grow the tree in the best-first fashion\n",
    "min_impurity_decrease= a node  will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
    "ccp_alpha= complexity parameter used for pruning\n",
    "CLASSIFIER ONLY\n",
    "class_weight= weights associated with the classes in dictionary form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTRIBUTES\n",
    "feature_importances_ : breakdown of all features and their relative importances\n",
    "estimator_ : the child estimator template used to create the collection of fitted sub-estimators; estimator used to grow the ensemble\n",
    "estimators_ : collection of fitted sub-estimators\n",
    "classes_ : class labels\n",
    "n_features_in_ : number of features seen during the .fit()\n",
    "feature_names_in_ : names of the above features\n",
    "n_outputs_ : number of outputs when .fit() is performed\n",
    "oob_score_ : score of the training dataset obtained using oob estimate\n",
    "oob_decision_function_ : decision function used with oob on the training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
